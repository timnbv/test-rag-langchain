{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\". Return result with Markdown formatting.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model (llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama # type: ignore\n",
    "llm = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Str Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = prompt | llm | parser\n",
    "# result = chain.invoke({\"context\": \"Peter loves Mary. Mary loves Silvia. Silvia loves Peter. Mary doesn't love Peter\",\n",
    "#                         \"question\": \"what is 1 + 1\"})\n",
    "# display(Markdown(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Comprehensive Overview from Training to Inference\\n4.2. Memory Scheduling\\nDeployingLLMsonasingleconsumer-gradeGPUisconstrainedbythelimitationsoftheavailablevideomemory,\\ngiventhesubstantialparametersofLLMs.Therefore,appropriateMemorySchedulingstrategiescanbeusedtosolve\\nthe hardware limitations of large model inference. Memory scheduling in large model inference involves the efficient\\norganization and management of memory access patterns during the reasoning or inference phase of complex neural\\nnetwork models. In the context of sophisticated reasoning tasks, such as natural language understanding or complex\\ndecision-making, large models often have intricate architectures and considerable memory requirements. Memory\\nschedulingoptimizestheretrievalandstorageofintermediaterepresentations,modelparameters,andactivationvalues,\\nensuring that the inference process is both accurate and performed with minimal latency. For example, BMInf [184]\\nutilizes the principle of virtual memory, achieving efficient inference for large models by intelligently scheduling the\\nparameters of each layer between the GPU and CPU.\\n4.3. Parallelism\\nBoth inference and training can leverage parallelization techniques. Presently, parallelization techniques for\\ninference primarily manifest across three dimensions: Data Parallelism, Tensor Parallelism, and Pipeline Parallelism.\\nData Parallelism primarily involves increasing the overall throughput of the inference system by adding more GPU\\ndevices [101; 97; 159; 185]. Tensor parallelism is a form of model parallelism where the model‚Äôs parameters are\\npartitioned into multiple tensors, each computed on different processing units. This approach proves beneficial when\\ndealing with models that are too large to fit into the memory of a single GPU. Tensor parallelism primarily involves\\nincreasingthenumberofdeviceshorizontallythroughparallelcomputationtoreducelatency[96].Pipelineparallelism\\nprimarily involves vertically increasing the number of GPU devices through parallel computation to support larger\\nmodelsandenhancedeviceutilization.Typically,itiscombinedwithtensorparallelismtoachieveoptimalperformance\\n[98].\\n4.4. Structural Optimization\\nIn the forward propagation computation of LLMs, the calculation speed is significantly faster than the speed\\nof memory access. Inference speed can be impacted by numerous memory access operations. One goal in LLM\\ninference is to minimize the number of memory accesses during forward propagation. FlashAttention [186] and\\nPagedAttention [187] enhance computational speed by employing a chunked computation approach, mitigating the\\nstorage overhead associated with matrices. The entire operation takes place within SRAM, reducing the number of\\naccesses to High Bandwidth Memory (HBM) and significantly boosting computational speed. Both FlashAttention\\nand PagedAttention have been adopted by mainstream inference frameworks, and seamlessly integrated into these\\nframeworks for straightforward utilization.\\n4.5. Inference Framework\\nParallelcomputing,modelcompression,memoryscheduling,andspecificoptimizationsfortransformerstructures,\\nall integral to LLM inference, have been effectively implemented in mainstream inference frameworks. These\\nframeworks furnish the foundational infrastructure and tools required for deploying and running LLM models. They\\noffer a spectrum of tools and interfaces, streamlining the deployment and inference processes for researchers and\\nengineers across diverse application scenarios. The choice of a framework typically hinges on project requirements,\\nhardware support, and user preferences. In Table 4, we compile some of these frameworks for reference.\\n5. Utilization of LLMs\\nThe application scope of LLMs is extensive and can be practically employed in almost any specialized domain\\n[1; 193; 46; 194; 195]. Following pre-training and fine-tuning, LLMs are primarily utilized by designing suitable', metadata={'page': 19, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nPre-trained language models (PLMs) with significantly larger parameter sizes and extensive training data are\\ntypically denoted as Large Language Models (LLMs) [15; 16; 17]. The model size usually exceeds 6-10 billion (6-\\n10B)parameters.AprominentmilestoneinthedevelopmentofLLMsisexemplifiedbytheGPTseries[18;7;8;19].\\nNotably, OpenAI released ChatGPT in November 2022, marking a pivotal moment in the era of LLMs and a game-\\nchanging moment in the field of artificial intelligence. ChatGPT has empowered current AI algorithms to achieve\\nunprecedented levels of strength and effectiveness, reshaping the way humans employ or develop AI algorithms.\\nIts emergence has captured the attention of the research community. However, owing to ChatGPT‚Äôs absence as an\\nopen-source platform, the principal way to use ChatGPT currently is by accessing it through OpenAI‚Äôs website at\\nhttps://chat.openai.com orviatheirAPIinterface.TrainingLLMsthatcanserveasalternativestoChatGPT,or\\ndomain-specific LLMs, has become highly necessary [20; 21; 22; 23; 24; 1; 25; 26]. Training and deploying LLMs\\ndemand expertise in handling large-scale data and substantial practical experience in distributed parallel training\\n[27;28;29].ThisrequirementemphasizestheneedforresearchersdevelopingLLMstopossesssignificantengineering\\ncapabilitiesinaddressingthechallengesencounteredduringLLMdevelopment.Researcherswhoareinterestedinthe\\nfield of LLMs must possess engineering skills or learn to collaborate effectively with engineers.\\nFortheabovereasons,theprimaryobjectiveofthispaperistoprovideacomprehensiveoverviewofLLMstraining\\nand inference techniques to equip researchers with the knowledge required for developing, deploying, and applying\\nLLMs. The structure of the rest of this review is as follows: In Section 2, we will introduce the relevant background\\nandfoundationalknowledgeofLLMs.InSection3,wewilldelveintothetechnicalaspectsoftrainingLLMs,whilein\\nSection 4 we will explore the technologies related to LLM‚Äôs inference and deployment. In Section 5, we will discuss\\nthe utilization of LLMs, and Section 6 will explore the future directions and their implications for LLMs.\\n2. Background Knowledge\\n2.1. Transformer\\nTransformer is a deep learning model based on an attention mechanism for processing sequence data that can\\neffectively solve complex natural language processing problems. This model was first proposed in 2017 [6], and\\nreplaced the traditional recurrent neural network architecture [30] in machine translation tasks as the state-of-the-art\\nmodel at that time. Due to its suitability for parallel computing and the complexity of the model itself, Transformer\\noutperformsthepreviouslypopularrecurrentneuralnetworksintermsofaccuracyandperformance.TheTransformer\\narchitecture consists primarily of two modules, an Encoder and a Decoder, as well as the attention mechanism within\\nthese modules.\\n2.1.1. Self-Attention\\nSelf-AttentionStructure[6]: Essentially,theattentionmechanismaimsatselectingasmallamountofimportant\\ninformation from a large amount of data and focusing on these important pieces while ignoring the majority of\\nunimportant information. The self-attention mechanism, as a variant of the attention mechanism, reduces reliance on\\nexternal information and excels at capturing internal correlations within data or features. Applying the self-attention\\nmechanismintext-primarilyinvolvescalculatingthemutualinfluencebetweenwordstoaddresstheissueoflong-range\\ndependencies.Additionally,self-attentionisthecoreideabehindtransformers.Thecoreformulaforkey-valueattention\\nis as follows:\\nùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ (ùëÑ, ùêæ, ùëâ ) =ùë†ùëúùëìùë°ùëöùëéùë• (ùëÑùêæùëá\\n‚àö\\nùëëùëò)ùëâ (1)\\nSelf-attention allows the model to weigh the importance of different words in a sentence when predicting a particular\\nword.Itcalculatesaweightedsumofthevaluesofallwordsinthesentence,wheretheweightsaredeterminedbythe\\nrelevance of each word to the target word.', metadata={'page': 1, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='Understanding LLMs: A Comprehensive Overview from Training\\nto Inference\\nYiheng Liua, Hao Hea, Tianle Hana, Xu Zhanga, Mengyuan Liua, Jiaming Tiana,\\nYutong Zhangb, Jiaqi Wangc, Xiaohui Gaod, Tianyang Zhongd, Yi Pane, Shaochen Xue,\\nZihao Wue, Zhengliang Liue, Xin Zhangb, Shu Zhangc, Xintao Hud, Tuo Zhangd,\\nNing Qianga, Tianming Liueand Bao Gea\\naSchool of Physics and Information Technology, Shaanxi Normal University, Xi‚Äôan, 710119, Shaanxi, China\\nbInstitute of Medical Research, Northwestern Polytechnical University, Xi‚Äôan, 710072, Shaanxi, China\\ncSchool of Computer Science, Northwestern Polytechnical University, Xi‚Äôan, 710072, Shaanxi, China\\ndSchool of Automation, Northwestern Polytechnical University, Xi‚Äôan, 710072, Shaanxi, China\\neSchool of Computing, The University of Georgia, Athens, 30602, USA\\nARTICLE INFO\\nKeywords :\\nLarge Language Models\\nTraining\\nInference\\nSurveyABSTRACT\\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large\\nLanguage Models (LLMs) for addressing downstream tasks. There‚Äôs an increasing focus on\\ncost-efficienttraininganddeploymentwithinthiscontext.Low-costtraininganddeploymentof\\nLLMsrepresentthefuturedevelopmenttrend.Thispaperreviewstheevolutionoflargelanguage\\nmodel training techniques and inference deployment technologies aligned with this emerging\\ntrend.Thediscussionontrainingincludesvariousaspects,includingdatapreprocessing,training\\narchitecture, pre-training tasks, parallel training, and relevant content related to model fine-\\ntuning. On the inference side, the paper covers topics such as model compression, parallel\\ncomputation,memoryscheduling,andstructuraloptimization.ItalsoexploresLLMs‚Äôutilization\\nand provides insights into their future development.\\n1. Introduction\\nLanguage modeling (LM) is a fundamental approach for achieving cognitive intelligence in the field of natural\\nlanguage processing (NLP), and its progress has been notable in recent years [1; 2; 3]. It assumes a central role\\nin understanding, generating, and manipulating human language, serving as the cornerstone for a diverse range of\\nNLP applications [4], including machine translation, chatbots, sentiment analysis, and text summarization. With\\nthe evolution of deep learning, the early statistical language models (SLM) have gradually transformed into neural\\nlanguage models (NLM) based on neural networks. This shift is characterized by the adoption of word embeddings,\\nrepresentingwordsasdistributedvectors.Notably,thesewordembeddingshaveconsistentlyexcelledinpracticalNLP\\ntasks, profoundly shaping the field‚Äôs progress. Pre-trained language models (PLM) represent a subsequent phase in\\nthe evolution of language models following NLM. Early attempts at PLMs included ELMo [5], which was built on a\\nBidirectionalLSTMarchitecture.However,withtheadventofthetransformerarchitecture[6],characterizedbyparallel\\nself-attention mechanisms, the pre-training and fine-tuning learning paradigm has propelled PLM to prominence as\\ntheprevailingapproach.Thesemodelsaretypicallytrainedviaself-supervisiononextensivedatasets,cementingtheir\\nstatus as the primary methodology in the field.\\nTheTransformerarchitectureisexceptionallywell-suitedforscalingupmodels,andresearchanalysishasrevealed\\nthat increasing the model‚Äôs scale or training data size can significantly enhance its performance. Many studies have\\npushed the boundaries of model performance by continuously expanding the scale of PLM [7; 8; 9; 10]. As models\\ngrow larger, a remarkable phenomenon known as \"emergence\" occurs, wherein they exhibit astonishing performance\\n[8].Thesemodelsarecapableofgeneratinghigh-qualitytextandpossessrobustlearningandreasoningabilities.They\\ncaneventacklefew-shotlearningtasksthroughin-contextlearning(ICL)[8].Thisremarkablecapabilityenablestheir\\nseamless application to a wide range of downstream tasks across diverse domains [11; 12; 13; 14].\\n‚àóCorresponding author\\nORCID(s):', metadata={'page': 0, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nTable 5\\nList of open source LLMs.\\nLLM Size (B) Links\\nT5 [68] 11B https://github.com/google-research/text-to-text-transfer-transformer\\nCodeGen [81] 16B https://github.com/salesforce/CodeGen\\nMOSS [203] 16B https://github.com/OpenLMLab/MOSS\\nGLM [37] 130B https://github.com/THUDM/GLM\\nChatGLM [37] 6B https://github.com/THUDM/ChatGLM3\\nChatYuan [204] 0.7B https://github.com/clue-ai/ChatYuan\\nOPT [83] 175B https://github.com/facebookresearch/metaseq\\nBLOOM [38] 176B https://huggingface.co/bigscience/bloom\\nLLaMA [9] 65B https://github.com/facebookresearch/llama\\nCodeGeeX [82] 13B https://github.com/THUDM/CodeGeeX\\nBaichuan [205] 13B https://github.com/baichuan-inc/Baichuan2\\nAquila 7B https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila\\nMiniGPT-4 [206] 25B https://github.com/Vision-CAIR/MiniGPT-4\\nVicuna [207] 13B https://github.com/lm-sys/FastChat\\nLLMs is expected to continue expanding, thereby augmenting their learning capabilities and overall performance.\\nMoreover, the majority of currently available LLMs are confined to a single natural language modality, lacking\\nextensions to process multimodal data such as images, videos, and speech. There is a potential future trajectory for\\nLLMs to evolve towards handling information beyond text, incorporating multimodal data like images and audio.\\nThisevolutionwouldempowermodelstocomprehensivelyunderstandandgeneratemultimodalcontent,significantly\\nbroadeningtheapplicationscopeofLLMs.TheinevitableexpansionofLLMsintothefieldofmultimodalityisbound\\ntoincurincreasedtrainingcosts.Apivotalfocusforfuturedevelopmentsliesintheefficientfine-tuningofparameters\\nandthedeploymentofLLMsthroughtechniquessuchasknowledgedistillation,modelcompression,andquantization,\\naimedatreducingboththetrainingandinferencecostsofLLMs.Anotheremergingtrendisthedomain-specifictraining\\nand fine-tuning of LLMs for particular sectors, facilitating a more adept adaptation to and understanding of industry-\\nspecific terminologies and contexts. Lastly, in the exploration of potential new architectures for LLMs the current\\nlandscape predominantly relies on the transformer architecture. While the transformer architecture naturally boasts\\nadvantages such as parallel computing and adaptability to various input modalities, its design typically necessitates\\nfixed-sizeinputs.Thisrequirementmaynecessitatepaddingortruncationwhendealingwithvariable-lengthsequences,\\npotentially leading to computational and information inefficiencies, as well as challenges in generating coherent data.\\nInvestigating the potential of Recurrent Neural Network (RNN) architectures in the era of LLMs could emerge as a\\npivotalresearchdirection.Forinstance,RWKV[208],anLLMdesignedundertheRNNarchitecture,hasdemonstrated\\ncompetitiveperformanceonvariousthird-partyevaluations,provingitselfcomparabletothemajorityoftransformer-\\nbased LLMs.\\nFor researchers in the field of AI, working in isolation is becoming increasingly impractical. The future direction\\nof AI development will intertwine with various industries, necessitating close collaboration with professionals from\\ndiversefields.Itiscrucialtoengageincollaborativeefforts,bridgingresearchdisciplines,andcollectivelyaddressing\\nchallengesbycombiningexpertisefromdifferentdomains.Simultaneously,thereisafreshsetofrequirementsforthe\\ncomprehensiveskillsofAIresearchers.TraininganddeployingLLMsnecessitateproficiencyinmanaginglarge-scale\\ndataandsubstantialpracticalexperienceindistributedparalleltraining.Thiscriterionunderscorestheimportancefor\\nresearchers involved in LLM development to possess substantial engineering capabilities, addressing the challenges\\ninherent in the process. Researchers who are interested in the field of LLMs must either possess engineering skills or\\nadeptly collaborate with engineers to navigate the complexities of model development [3].\\nAs LLMs find widespread applications in societal life, concerns about ethical issues and societal impact are on a', metadata={'page': 21, 'source': 'docs/Understanding_LLMs.pdf'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "chroma = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "chroma.similarity_search(\"Why LLMs are so popular?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain = (\n",
    "    {\"context\": chroma.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | parser\n",
    "  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the context, it seems that Large Language Models (LLMs) are gaining popularity due to their ability to continue expanding and augmenting their learning capabilities, thereby increasing their overall performance. Additionally, the majority of currently available LLMs are confined to a single natural language modality, which is expected to evolve towards handling multimodal data like images and audio in the future. This evolution would enable models to comprehensively understand and generate multimodal content, significantly broadening the applications scope of LLMs.\n",
       "\n",
       "Furthermore, researchers in the field of AI working on LLM development are recognizing the importance of collaboration with professionals from diverse fields to address the challenges inherent in developing these complex models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chain.invoke(\"Why LLMs are so popular?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the context provided, a Transformer is a type of neural network architecture that allows for parallel processing and self-attention mechanisms to handle complex patterns in sequential data such as text.\n",
       "\n",
       "In particular, it calculates a weighted sum of all words in a sentence where the weights are determined by the relevance of each word to the target word. This allows the model to weigh the importance of different words in a sentence when predicting a particular word."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(chain.invoke(\"Describe what is a transformer?\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing wrong question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'm not able to answer your question based on the context provided. It seems that the document is related to large language models and their applications in various fields, but it does not contain information about the first astronauts landing on the moon.\n",
       "\n",
       "The document appears to be a collection of research papers and preprints on topics such as natural language processing, mathematical problem-solving, and question answering. If you're looking for information about the first astronauts to land on the moon, I'd be happy to help you with that!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "result = chain.invoke(\"When first astronauts landed on the moon?\")\n",
    "display(Markdown(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
