{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings (Ollama: nomic-embed-text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5855991840362549,\n",
       " 0.4734927713871002,\n",
       " -3.261312961578369,\n",
       " -0.27085670828819275,\n",
       " 2.32326078414917]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "query_result = embeddings.embed_query(\"Test query.\")\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Understanding LLMs: A Comprehensive Overview from Training\\nto Inference\\nYiheng Liua, Hao Hea, Tianle Hana, Xu Zhanga, Mengyuan Liua, Jiaming Tiana,\\nYutong Zhangb, Jiaqi Wangc, Xiaohui Gaod, Tianyang Zhongd, Yi Pane, Shaochen Xue,\\nZihao Wue, Zhengliang Liue, Xin Zhangb, Shu Zhangc, Xintao Hud, Tuo Zhangd,\\nNing Qianga, Tianming Liueand Bao Gea\\naSchool of Physics and Information Technology, Shaanxi Normal University, Xi’an, 710119, Shaanxi, China\\nbInstitute of Medical Research, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China\\ncSchool of Computer Science, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China\\ndSchool of Automation, Northwestern Polytechnical University, Xi’an, 710072, Shaanxi, China\\neSchool of Computing, The University of Georgia, Athens, 30602, USA\\nARTICLE INFO\\nKeywords :\\nLarge Language Models\\nTraining\\nInference\\nSurveyABSTRACT\\nThe introduction of ChatGPT has led to a significant increase in the utilization of Large\\nLanguage Models (LLMs) for addressing downstream tasks. There’s an increasing focus on\\ncost-efficienttraininganddeploymentwithinthiscontext.Low-costtraininganddeploymentof\\nLLMsrepresentthefuturedevelopmenttrend.Thispaperreviewstheevolutionoflargelanguage\\nmodel training techniques and inference deployment technologies aligned with this emerging\\ntrend.Thediscussionontrainingincludesvariousaspects,includingdatapreprocessing,training\\narchitecture, pre-training tasks, parallel training, and relevant content related to model fine-\\ntuning. On the inference side, the paper covers topics such as model compression, parallel\\ncomputation,memoryscheduling,andstructuraloptimization.ItalsoexploresLLMs’utilization\\nand provides insights into their future development.\\n1. Introduction\\nLanguage modeling (LM) is a fundamental approach for achieving cognitive intelligence in the field of natural\\nlanguage processing (NLP), and its progress has been notable in recent years [1; 2; 3]. It assumes a central role\\nin understanding, generating, and manipulating human language, serving as the cornerstone for a diverse range of\\nNLP applications [4], including machine translation, chatbots, sentiment analysis, and text summarization. With\\nthe evolution of deep learning, the early statistical language models (SLM) have gradually transformed into neural\\nlanguage models (NLM) based on neural networks. This shift is characterized by the adoption of word embeddings,\\nrepresentingwordsasdistributedvectors.Notably,thesewordembeddingshaveconsistentlyexcelledinpracticalNLP\\ntasks, profoundly shaping the field’s progress. Pre-trained language models (PLM) represent a subsequent phase in\\nthe evolution of language models following NLM. Early attempts at PLMs included ELMo [5], which was built on a\\nBidirectionalLSTMarchitecture.However,withtheadventofthetransformerarchitecture[6],characterizedbyparallel\\nself-attention mechanisms, the pre-training and fine-tuning learning paradigm has propelled PLM to prominence as\\ntheprevailingapproach.Thesemodelsaretypicallytrainedviaself-supervisiononextensivedatasets,cementingtheir\\nstatus as the primary methodology in the field.\\nTheTransformerarchitectureisexceptionallywell-suitedforscalingupmodels,andresearchanalysishasrevealed\\nthat increasing the model’s scale or training data size can significantly enhance its performance. Many studies have\\npushed the boundaries of model performance by continuously expanding the scale of PLM [7; 8; 9; 10]. As models\\ngrow larger, a remarkable phenomenon known as \"emergence\" occurs, wherein they exhibit astonishing performance\\n[8].Thesemodelsarecapableofgeneratinghigh-qualitytextandpossessrobustlearningandreasoningabilities.They\\ncaneventacklefew-shotlearningtasksthroughin-contextlearning(ICL)[8].Thisremarkablecapabilityenablestheir\\nseamless application to a wide range of downstream tasks across diverse domains [11; 12; 13; 14].\\n∗Corresponding author\\nORCID(s):', metadata={'source': 'docs/Understanding_LLMs.pdf', 'page': 0}),\n",
       " Document(page_content='seamless application to a wide range of downstream tasks across diverse domains [11; 12; 13; 14].\\n∗Corresponding author\\nORCID(s):\\nYiheng Liu et al.: Preprint submitted to Elsevier Page 1 of 30arXiv:2401.02038v2  [cs.CL]  6 Jan 2024', metadata={'source': 'docs/Understanding_LLMs.pdf', 'page': 0}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nPre-trained language models (PLMs) with significantly larger parameter sizes and extensive training data are\\ntypically denoted as Large Language Models (LLMs) [15; 16; 17]. The model size usually exceeds 6-10 billion (6-\\n10B)parameters.AprominentmilestoneinthedevelopmentofLLMsisexemplifiedbytheGPTseries[18;7;8;19].\\nNotably, OpenAI released ChatGPT in November 2022, marking a pivotal moment in the era of LLMs and a game-\\nchanging moment in the field of artificial intelligence. ChatGPT has empowered current AI algorithms to achieve\\nunprecedented levels of strength and effectiveness, reshaping the way humans employ or develop AI algorithms.\\nIts emergence has captured the attention of the research community. However, owing to ChatGPT’s absence as an\\nopen-source platform, the principal way to use ChatGPT currently is by accessing it through OpenAI’s website at\\nhttps://chat.openai.com orviatheirAPIinterface.TrainingLLMsthatcanserveasalternativestoChatGPT,or\\ndomain-specific LLMs, has become highly necessary [20; 21; 22; 23; 24; 1; 25; 26]. Training and deploying LLMs\\ndemand expertise in handling large-scale data and substantial practical experience in distributed parallel training\\n[27;28;29].ThisrequirementemphasizestheneedforresearchersdevelopingLLMstopossesssignificantengineering\\ncapabilitiesinaddressingthechallengesencounteredduringLLMdevelopment.Researcherswhoareinterestedinthe\\nfield of LLMs must possess engineering skills or learn to collaborate effectively with engineers.\\nFortheabovereasons,theprimaryobjectiveofthispaperistoprovideacomprehensiveoverviewofLLMstraining\\nand inference techniques to equip researchers with the knowledge required for developing, deploying, and applying\\nLLMs. The structure of the rest of this review is as follows: In Section 2, we will introduce the relevant background\\nandfoundationalknowledgeofLLMs.InSection3,wewilldelveintothetechnicalaspectsoftrainingLLMs,whilein\\nSection 4 we will explore the technologies related to LLM’s inference and deployment. In Section 5, we will discuss\\nthe utilization of LLMs, and Section 6 will explore the future directions and their implications for LLMs.\\n2. Background Knowledge\\n2.1. Transformer\\nTransformer is a deep learning model based on an attention mechanism for processing sequence data that can\\neffectively solve complex natural language processing problems. This model was first proposed in 2017 [6], and\\nreplaced the traditional recurrent neural network architecture [30] in machine translation tasks as the state-of-the-art\\nmodel at that time. Due to its suitability for parallel computing and the complexity of the model itself, Transformer\\noutperformsthepreviouslypopularrecurrentneuralnetworksintermsofaccuracyandperformance.TheTransformer\\narchitecture consists primarily of two modules, an Encoder and a Decoder, as well as the attention mechanism within\\nthese modules.\\n2.1.1. Self-Attention\\nSelf-AttentionStructure[6]: Essentially,theattentionmechanismaimsatselectingasmallamountofimportant\\ninformation from a large amount of data and focusing on these important pieces while ignoring the majority of\\nunimportant information. The self-attention mechanism, as a variant of the attention mechanism, reduces reliance on\\nexternal information and excels at capturing internal correlations within data or features. Applying the self-attention\\nmechanismintext-primarilyinvolvescalculatingthemutualinfluencebetweenwordstoaddresstheissueoflong-range\\ndependencies.Additionally,self-attentionisthecoreideabehindtransformers.Thecoreformulaforkey-valueattention\\nis as follows:\\n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄, 𝐾, 𝑉 ) =𝑠𝑜𝑓𝑡𝑚𝑎𝑥 (𝑄𝐾𝑇\\n√\\n𝑑𝑘)𝑉 (1)\\nSelf-attention allows the model to weigh the importance of different words in a sentence when predicting a particular\\nword.Itcalculatesaweightedsumofthevaluesofallwordsinthesentence,wheretheweightsaredeterminedbythe\\nrelevance of each word to the target word.', metadata={'source': 'docs/Understanding_LLMs.pdf', 'page': 1}),\n",
       " Document(page_content='word.Itcalculatesaweightedsumofthevaluesofallwordsinthesentence,wheretheweightsaredeterminedbythe\\nrelevance of each word to the target word.\\nTheself-attentionmechanismconsistsofthreesteps:calculatingthequery,key,andvaluevectors.Thequeryvector\\nrepresentsthewordbeingattendedto,whilethekeyvectorsrepresentallthewordsinthesentence.Thevaluevectors\\nstoretheinformationassociatedwitheachword.Theattentionweightsarecomputedbytakingthedotproductbetween\\nthe query and key vectors, followed by a softmax operation to obtain a distribution over the words.\\nMulti-Head Attention [6]: Multi-head self-attention extends the self-attention mechanism by performing it\\nmultiple times in parallel. Each attention head learns to focus on different aspects of the input, capturing different\\ndependenciesandpatterns.Theoutputsoftheattentionheadsarethenconcatenatedandlinearlytransformedtoobtain\\nYiheng Liu et al.: Preprint submitted to Elsevier Page 2 of 30', metadata={'source': 'docs/Understanding_LLMs.pdf', 'page': 1}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nthefinalrepresentation.Byusingmultipleattentionheads,themodelcancapturebothlocalandglobaldependencies,\\nallowingforamorecomprehensiveunderstandingoftheinputsequence.Thisparallelizationalsoenhancesthemodel’s\\ncapacity to capture complex relationships between words. The Multi-head attention can be formulated as follows:\\n𝑀𝑢𝑙𝑡𝑖𝐻𝑒𝑎𝑑𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄, 𝐾, 𝑉 ) =𝐶𝑜𝑛𝑐𝑎𝑡 [ℎ𝑒𝑎𝑑1,…, ℎ𝑒𝑎𝑑ℎ]𝑊𝑜\\n𝑤ℎ𝑒𝑟𝑒 ℎ𝑒𝑎𝑑𝑖=𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄𝑊𝑄\\n𝑖, 𝐾𝑊𝐾\\n𝑖, 𝑉 𝑊𝑉\\n𝑖)(2)\\nIn this case, \" 𝐶𝑜𝑛𝑐𝑎𝑡\" means to concatenate the attention calculation results of each head, \" 𝑊𝑜\" is the weight\\nmatrix of the output layer, used to linearly transform the concatenated results. This yields the output of multi-head\\nattention. In summary, multi-head attention enhances the model’s ability to represent input sequences by performing\\nparallelattentioncalculationsunderdifferentlineartransformations,thenconcatenatingandlinearlytransformingthe\\nresults.ThismechanismplaysanimportantroleintheTransformermodel,helpingtohandlelong-rangedependencies\\nand improve model performance.\\n2.1.2. Encoder\\nTheencodermodule[6]oftheTransformermodeliscomposedofmultipleidenticallayers,eachofwhichincludes\\namulti-headattentionmechanismandfeed-forwardneuralnetwork[31].Inthemulti-headattentionmechanism,each\\nposition in the input sequence is calculated for attention with other positions to capture the dependencies between\\ndifferent positions in the input sequence. The feed-forward neural network is then used to further process and extract\\nfeatures from the output of the attention mechanism. The encoder module gradually extracts features of the input\\nsequence through the stacking of multiple such layers and passes the final encoding result to the decoder module for\\ndecoding.Thedesignoftheencodermoduleenablesittoeffectivelyhandlelong-rangedependencieswithintheinput\\nsequence and has significantly improved performance in various NLP tasks.\\n2.1.3. Decoder\\nThe decoder module [32] of the Transformer model is also composed of multiple identical layers, each of which\\nincludes a multi-head attention mechanism and a feed-forward neural network. Unlike the encoder, the decoder also\\nincludes an additional encoder-decoder attention mechanism, used to compute attention on the input sequence during\\nthe decoding process. At each position, the decoder can only perform self-attention calculations with the positions\\nbefore it to ensure that the generation of the sequence does not violate grammar rules. Masks play an important role\\nin the decoder, ensuring that only information before the current time step is focused on when generating the output\\nsequence, and not leaking information from future time steps. Specifically, the decoder’s self-attention mechanism\\nuses masks to prevent the model from accessing future information when generating predictions at each time step,\\nmaintainingthecausalityofthemodel.Thisensuresthattheoutputgeneratedbythemodeldependsontheinformation\\nat the current time step and before, without being influenced by future information.\\n2.1.4. Positional Embedding\\nPosition and order are crucial for certain tasks, such as understanding a sentence or a video. Position and order\\ndefinethegrammarofasentence,theyareintegraltothesemanticsofsentences.TheTransformerutilizesMulti-Head\\nSelf-Attention (MHSA) to avoid the recursive approach of RNN, thus speeding up the training process. Additionally,\\nit can capture long-range dependencies in sentences and handle longer inputs. When each token in a sentence passes\\nthrough the Transformer’s Encoder/Decoder stack, the model itself lacks any sense of position/order for each token\\n(permutation invariance). Therefore, a method is still needed to incorporate the sequential information of tokens into\\nthe model. To enable the model to perceive the input sequence, positional information about the location of each\\ntoken in the sentence can be added, and this technique is known as positional embedding (PE). which is used in the', metadata={'source': 'docs/Understanding_LLMs.pdf', 'page': 2})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"docs/Understanding_LLMs.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chroma db setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain_chroma import Chroma\n",
    "if os.path.exists('./chroma_db'):\n",
    "    shutil.rmtree('./chroma_db')\n",
    "db = Chroma.from_documents(pages, embeddings, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Comprehensive Overview from Training to Inference\\nTable 5\\nList of open source LLMs.\\nLLM Size (B) Links\\nT5 [68] 11B https://github.com/google-research/text-to-text-transfer-transformer\\nCodeGen [81] 16B https://github.com/salesforce/CodeGen\\nMOSS [203] 16B https://github.com/OpenLMLab/MOSS\\nGLM [37] 130B https://github.com/THUDM/GLM\\nChatGLM [37] 6B https://github.com/THUDM/ChatGLM3\\nChatYuan [204] 0.7B https://github.com/clue-ai/ChatYuan\\nOPT [83] 175B https://github.com/facebookresearch/metaseq\\nBLOOM [38] 176B https://huggingface.co/bigscience/bloom\\nLLaMA [9] 65B https://github.com/facebookresearch/llama\\nCodeGeeX [82] 13B https://github.com/THUDM/CodeGeeX\\nBaichuan [205] 13B https://github.com/baichuan-inc/Baichuan2\\nAquila 7B https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila\\nMiniGPT-4 [206] 25B https://github.com/Vision-CAIR/MiniGPT-4\\nVicuna [207] 13B https://github.com/lm-sys/FastChat\\nLLMs is expected to continue expanding, thereby augmenting their learning capabilities and overall performance.\\nMoreover, the majority of currently available LLMs are confined to a single natural language modality, lacking\\nextensions to process multimodal data such as images, videos, and speech. There is a potential future trajectory for\\nLLMs to evolve towards handling information beyond text, incorporating multimodal data like images and audio.\\nThisevolutionwouldempowermodelstocomprehensivelyunderstandandgeneratemultimodalcontent,significantly\\nbroadeningtheapplicationscopeofLLMs.TheinevitableexpansionofLLMsintothefieldofmultimodalityisbound\\ntoincurincreasedtrainingcosts.Apivotalfocusforfuturedevelopmentsliesintheefficientfine-tuningofparameters\\nandthedeploymentofLLMsthroughtechniquessuchasknowledgedistillation,modelcompression,andquantization,\\naimedatreducingboththetrainingandinferencecostsofLLMs.Anotheremergingtrendisthedomain-specifictraining\\nand fine-tuning of LLMs for particular sectors, facilitating a more adept adaptation to and understanding of industry-\\nspecific terminologies and contexts. Lastly, in the exploration of potential new architectures for LLMs the current\\nlandscape predominantly relies on the transformer architecture. While the transformer architecture naturally boasts\\nadvantages such as parallel computing and adaptability to various input modalities, its design typically necessitates\\nfixed-sizeinputs.Thisrequirementmaynecessitatepaddingortruncationwhendealingwithvariable-lengthsequences,\\npotentially leading to computational and information inefficiencies, as well as challenges in generating coherent data.\\nInvestigating the potential of Recurrent Neural Network (RNN) architectures in the era of LLMs could emerge as a\\npivotalresearchdirection.Forinstance,RWKV[208],anLLMdesignedundertheRNNarchitecture,hasdemonstrated\\ncompetitiveperformanceonvariousthird-partyevaluations,provingitselfcomparabletothemajorityoftransformer-\\nbased LLMs.\\nFor researchers in the field of AI, working in isolation is becoming increasingly impractical. The future direction\\nof AI development will intertwine with various industries, necessitating close collaboration with professionals from\\ndiversefields.Itiscrucialtoengageincollaborativeefforts,bridgingresearchdisciplines,andcollectivelyaddressing\\nchallengesbycombiningexpertisefromdifferentdomains.Simultaneously,thereisafreshsetofrequirementsforthe\\ncomprehensiveskillsofAIresearchers.TraininganddeployingLLMsnecessitateproficiencyinmanaginglarge-scale\\ndataandsubstantialpracticalexperienceindistributedparalleltraining.Thiscriterionunderscorestheimportancefor\\nresearchers involved in LLM development to possess substantial engineering capabilities, addressing the challenges\\ninherent in the process. Researchers who are interested in the field of LLMs must either possess engineering skills or\\nadeptly collaborate with engineers to navigate the complexities of model development [3].\\nAs LLMs find widespread applications in societal life, concerns about ethical issues and societal impact are on a', metadata={'page': 21, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nTable 1\\nCommonly used corpora information.\\nCorpora Type Links\\nBookCorpus [65] Books https://github.com/soskek/bookcorpus\\nGutenberg [66] Books https://www.gutenberg.org\\nBooks1 [8] Books Not open source yet\\nBooks2 [8] Books Not open source yet\\nCommonCrawl [67] CommonCrawl https://commoncrawl.org\\nC4 [68] CommonCrawl https://www.tensorflow.org/datasets/catalog/c4\\nCC-Stories [69] CommonCrawl Not open source yet\\nCC-News [70] CommonCrawl https://commoncrawl.org/blog/news-dataset-available\\nRealNews [71] CommonCrawl https://github.com/rowanz/grover/tree/master/realnews\\nRefinedWeb [72] CommonCrawl https://huggingface.co/datasets/tiiuae/falcon-refinedweb\\nWebText Reddit Link Not open source yet\\nOpenWebText [73] Reddit Link https://skylion007.github.io/OpenWebTextCorpus/\\nPushShift.io [74] Reddit Link https://pushshift.io/\\nWikipedia [75] Wikipedia https://dumps.wikimedia.org/zhwiki/latest/\\nBigQuery [76] Code https://cloud.google.com/bigquery\\nCodeParrot Code https://huggingface.co/codeparrot\\nthe Pile [77] Other https://github.com/EleutherAI/the-pile\\nROOTS [78] Other https://huggingface.co/bigscience-data\\nBooks:TwocommonlyutilizedbooksdatasetsforLLMstrainingareBookCorpus[65]andGutenberg[66].These\\ndatasetsincludeawiderangeofliterarygenres,includingnovels,essays,poetry,history,science,philosophy,andmore.\\nWidely employed by numerous LLMs [9; 79], these datasets contribute to the models’ training by exposing them to a\\ndiverse array of textual genres and subject matter, fostering a more comprehensive understanding of language across\\nvarious domains.\\nCommonCrawl: CommonCrawl [67] manages an accessible repository of web crawl data, freely available for\\nutilization by individuals and organizations. This repository encompasses a vast collection of data, comprising over\\n250 billion web pages accumulated over a span of 16 years. Established in 2007, Common Crawl has evolved into a\\nwidelyrecognizedandreferencedcorpusintheacademicandresearchcommunities,citedinmorethan10,000research\\npapers.Thiscontinuouslyexpandingcorpusisadynamicresource,withanadditionof3–5billionnewwebpageseach\\nmonth.Itssignificanceextendstothefieldofnaturallanguageprocessing,whereitservesasaprimarytrainingcorpus\\nin numerous large language models. Notably, a substantial portion of the raw tokens employed in training GPT-3\\n[8], amounting to 82%, is sourced from the CommonCrawl. However, due to the presence of a substantial amount of\\nlow-quality data in web archives, preprocessing is essential when working with CommonCrawl data. Currently, four\\ncommonlyusedfiltereddatasetsbasedonCommonCrawlareavailable:C4[68],CC-Stories[69],CC-News[70],and\\nRealNews [71].\\nReddit Links: Reddit is a social media platform where users can submit links and posts, and others can vote on\\nthemusingthe\"upvote\"or\"downvote\"system.Thischaracteristicmakesitavaluableresourceforcreatinghigh-quality\\ndatasets.\\nWikipedia: Wikipedia [75], a free and open online encyclopedia project, hosts a vast repository of high-quality\\nencyclopedic content spanning a wide array of topics. The English version of Wikipedia is extensively utilized in the\\ntraining of many LLMs [8; 9; 80], serving as a valuable resource for language understanding and generation tasks.\\nAdditionally, Wikipedia is available in multiple languages, providing diverse language versions that can be leveraged\\nfor training in multilingual environments.\\nCode:There is a limited availability of publicly accessible code datasets at present. Existing efforts primarily\\ninvolvewebscrapingofcodewithopen-sourcelicensesfromtheinternet.ThemainsourcesincludeGithubandStack\\nOverflow.\\nWe have organized datasets utilized by distinct LLMs. During the training process, LLMs are typically trained on\\nmultiple datasets, as specified in Table 2 for reference.\\nYiheng Liu et al.: Preprint submitted to Elsevier Page 7 of 30', metadata={'page': 6, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\n4.2. Memory Scheduling\\nDeployingLLMsonasingleconsumer-gradeGPUisconstrainedbythelimitationsoftheavailablevideomemory,\\ngiventhesubstantialparametersofLLMs.Therefore,appropriateMemorySchedulingstrategiescanbeusedtosolve\\nthe hardware limitations of large model inference. Memory scheduling in large model inference involves the efficient\\norganization and management of memory access patterns during the reasoning or inference phase of complex neural\\nnetwork models. In the context of sophisticated reasoning tasks, such as natural language understanding or complex\\ndecision-making, large models often have intricate architectures and considerable memory requirements. Memory\\nschedulingoptimizestheretrievalandstorageofintermediaterepresentations,modelparameters,andactivationvalues,\\nensuring that the inference process is both accurate and performed with minimal latency. For example, BMInf [184]\\nutilizes the principle of virtual memory, achieving efficient inference for large models by intelligently scheduling the\\nparameters of each layer between the GPU and CPU.\\n4.3. Parallelism\\nBoth inference and training can leverage parallelization techniques. Presently, parallelization techniques for\\ninference primarily manifest across three dimensions: Data Parallelism, Tensor Parallelism, and Pipeline Parallelism.\\nData Parallelism primarily involves increasing the overall throughput of the inference system by adding more GPU\\ndevices [101; 97; 159; 185]. Tensor parallelism is a form of model parallelism where the model’s parameters are\\npartitioned into multiple tensors, each computed on different processing units. This approach proves beneficial when\\ndealing with models that are too large to fit into the memory of a single GPU. Tensor parallelism primarily involves\\nincreasingthenumberofdeviceshorizontallythroughparallelcomputationtoreducelatency[96].Pipelineparallelism\\nprimarily involves vertically increasing the number of GPU devices through parallel computation to support larger\\nmodelsandenhancedeviceutilization.Typically,itiscombinedwithtensorparallelismtoachieveoptimalperformance\\n[98].\\n4.4. Structural Optimization\\nIn the forward propagation computation of LLMs, the calculation speed is significantly faster than the speed\\nof memory access. Inference speed can be impacted by numerous memory access operations. One goal in LLM\\ninference is to minimize the number of memory accesses during forward propagation. FlashAttention [186] and\\nPagedAttention [187] enhance computational speed by employing a chunked computation approach, mitigating the\\nstorage overhead associated with matrices. The entire operation takes place within SRAM, reducing the number of\\naccesses to High Bandwidth Memory (HBM) and significantly boosting computational speed. Both FlashAttention\\nand PagedAttention have been adopted by mainstream inference frameworks, and seamlessly integrated into these\\nframeworks for straightforward utilization.\\n4.5. Inference Framework\\nParallelcomputing,modelcompression,memoryscheduling,andspecificoptimizationsfortransformerstructures,\\nall integral to LLM inference, have been effectively implemented in mainstream inference frameworks. These\\nframeworks furnish the foundational infrastructure and tools required for deploying and running LLM models. They\\noffer a spectrum of tools and interfaces, streamlining the deployment and inference processes for researchers and\\nengineers across diverse application scenarios. The choice of a framework typically hinges on project requirements,\\nhardware support, and user preferences. In Table 4, we compile some of these frameworks for reference.\\n5. Utilization of LLMs\\nThe application scope of LLMs is extensive and can be practically employed in almost any specialized domain\\n[1; 193; 46; 194; 195]. Following pre-training and fine-tuning, LLMs are primarily utilized by designing suitable', metadata={'page': 19, 'source': 'docs/Understanding_LLMs.pdf'}),\n",
       " Document(page_content='A Comprehensive Overview from Training to Inference\\nPre-trained language models (PLMs) with significantly larger parameter sizes and extensive training data are\\ntypically denoted as Large Language Models (LLMs) [15; 16; 17]. The model size usually exceeds 6-10 billion (6-\\n10B)parameters.AprominentmilestoneinthedevelopmentofLLMsisexemplifiedbytheGPTseries[18;7;8;19].\\nNotably, OpenAI released ChatGPT in November 2022, marking a pivotal moment in the era of LLMs and a game-\\nchanging moment in the field of artificial intelligence. ChatGPT has empowered current AI algorithms to achieve\\nunprecedented levels of strength and effectiveness, reshaping the way humans employ or develop AI algorithms.\\nIts emergence has captured the attention of the research community. However, owing to ChatGPT’s absence as an\\nopen-source platform, the principal way to use ChatGPT currently is by accessing it through OpenAI’s website at\\nhttps://chat.openai.com orviatheirAPIinterface.TrainingLLMsthatcanserveasalternativestoChatGPT,or\\ndomain-specific LLMs, has become highly necessary [20; 21; 22; 23; 24; 1; 25; 26]. Training and deploying LLMs\\ndemand expertise in handling large-scale data and substantial practical experience in distributed parallel training\\n[27;28;29].ThisrequirementemphasizestheneedforresearchersdevelopingLLMstopossesssignificantengineering\\ncapabilitiesinaddressingthechallengesencounteredduringLLMdevelopment.Researcherswhoareinterestedinthe\\nfield of LLMs must possess engineering skills or learn to collaborate effectively with engineers.\\nFortheabovereasons,theprimaryobjectiveofthispaperistoprovideacomprehensiveoverviewofLLMstraining\\nand inference techniques to equip researchers with the knowledge required for developing, deploying, and applying\\nLLMs. The structure of the rest of this review is as follows: In Section 2, we will introduce the relevant background\\nandfoundationalknowledgeofLLMs.InSection3,wewilldelveintothetechnicalaspectsoftrainingLLMs,whilein\\nSection 4 we will explore the technologies related to LLM’s inference and deployment. In Section 5, we will discuss\\nthe utilization of LLMs, and Section 6 will explore the future directions and their implications for LLMs.\\n2. Background Knowledge\\n2.1. Transformer\\nTransformer is a deep learning model based on an attention mechanism for processing sequence data that can\\neffectively solve complex natural language processing problems. This model was first proposed in 2017 [6], and\\nreplaced the traditional recurrent neural network architecture [30] in machine translation tasks as the state-of-the-art\\nmodel at that time. Due to its suitability for parallel computing and the complexity of the model itself, Transformer\\noutperformsthepreviouslypopularrecurrentneuralnetworksintermsofaccuracyandperformance.TheTransformer\\narchitecture consists primarily of two modules, an Encoder and a Decoder, as well as the attention mechanism within\\nthese modules.\\n2.1.1. Self-Attention\\nSelf-AttentionStructure[6]: Essentially,theattentionmechanismaimsatselectingasmallamountofimportant\\ninformation from a large amount of data and focusing on these important pieces while ignoring the majority of\\nunimportant information. The self-attention mechanism, as a variant of the attention mechanism, reduces reliance on\\nexternal information and excels at capturing internal correlations within data or features. Applying the self-attention\\nmechanismintext-primarilyinvolvescalculatingthemutualinfluencebetweenwordstoaddresstheissueoflong-range\\ndependencies.Additionally,self-attentionisthecoreideabehindtransformers.Thecoreformulaforkey-valueattention\\nis as follows:\\n𝐴𝑡𝑡𝑒𝑛𝑡𝑖𝑜𝑛 (𝑄, 𝐾, 𝑉 ) =𝑠𝑜𝑓𝑡𝑚𝑎𝑥 (𝑄𝐾𝑇\\n√\\n𝑑𝑘)𝑉 (1)\\nSelf-attention allows the model to weigh the importance of different words in a sentence when predicting a particular\\nword.Itcalculatesaweightedsumofthevaluesofallwordsinthesentence,wheretheweightsaredeterminedbythe\\nrelevance of each word to the target word.', metadata={'page': 1, 'source': 'docs/Understanding_LLMs.pdf'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# query it\n",
    "query = \"What is LLM?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
